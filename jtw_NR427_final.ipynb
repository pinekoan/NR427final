{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=orange>**Cameron Peak Fire** </font>\n",
    "***\n",
    "## an analysis exploring high severity burned area on steep slopes <br> including potentially affected <font color=darkblue>watersheds</font>, with associated <font color=lightblue>streams</font> and <font color=blue>lakes</font>\n",
    "#### *a notebook by Jesse Wooten, last edited on 12/14/20*\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/ZYJfwH8GLCs4VHVstA/giphy.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Jesse Wooten\n",
    "# for NR427 final projects\n",
    "# December 14th, 2020\n",
    "\n",
    "# This script creates a layer for high severity burned area from BAER data on the Cameron Peak Fire\n",
    "# as well as a layer for steep slopes (> 30 %) derived from colorado DEM data\n",
    "# finds the intersection of those two layers, where erosion risk is high\n",
    "# the script then identifies which watersheds contain this high risk area\n",
    "# as well as locating which streams flow through the risk zone, and which lakes are in affected watersheds\n",
    "# two .csv reports and one .pdf plot of findings are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing a few libraries here\n",
    "# i install all libraries the script uses, just in case you don't have some of them installed already \n",
    "\n",
    "!pip install descartes\n",
    "!pip install adjustText\n",
    "!pip install earthpy\n",
    "!pip install pycrs\n",
    "!pip install richdem\n",
    "!pip install numpy\n",
    "!pip install fiona\n",
    "!pip install rasterio\n",
    "!pip install shapely\n",
    "!pip install os\n",
    "!pip install requests\n",
    "!pip install zipfile\n",
    "!pip install json\n",
    "!pip install ogr\n",
    "!pip install gdal\n",
    "!pip install matplotlib\n",
    "!pip install pycrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we import\n",
    "\n",
    "import richdem as rd\n",
    "import adjustText as aT\n",
    "import geopandas as gpd\n",
    "import earthpy.spatial as es\n",
    "import fiona, rasterio, shapely, os, requests, zipfile, json, matplotlib, descartes, pycrs, numpy, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more specific importing\n",
    "\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import box\n",
    "from fiona.crs import from_epsg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy import zeros\n",
    "from numpy import logical_and\n",
    "from osgeo import gdal, ogr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's access a few layers that are hosted online \n",
    "> #### go go gadget <font color=lightblue>streams</font> , <font color=blue>lakes</font> and <font color=darkblue>watersheds</font>!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create data frames for some data we need\n",
    "\n",
    "# Streams of Colorado\n",
    "\n",
    "COstreamsURL = r'https://data.colorado.gov/api/geospatial/x238-vje7?method=export&format=GeoJSON'\n",
    "COstreams = gpd.read_file(COstreamsURL)\n",
    "\n",
    "print(\"CRS for Colorado Streams is \" + str(COstreams.crs))\n",
    "\n",
    "COstreams.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lakes\n",
    "\n",
    "COlakesURL = r'https://data.colorado.gov/api/geospatial/uksn-8qya?method=export&format=GeoJSON'\n",
    "COlakes = gpd.read_file(COlakesURL)\n",
    "\n",
    "COlakes.plot()\n",
    "print(\"CRS for Colorado Lakes is \" + str(COlakes.crs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create watersheds\n",
    "\n",
    "COwsURL = r'https://geo.colorado.edu/download/file/47540-5ca269dcbfd55f000a1af25b-geojson.json'\n",
    "COws = gpd.read_file(COwsURL)\n",
    "\n",
    "COws.plot()\n",
    "print(\"CRS for Colorado Watersheds is \" + str(COws.crs))\n",
    "COws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's download a few zip files containing data we need\n",
    "> #### go go gadget <font color=orange>*BAER burn severity* </font> data and <font color=pink>*Colorado DEM* </font>  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now... we are looking at all of the streams, lakes, and watersheds in the entire state of Colorado\n",
    "# Seems like that scope may be a bit excessive for a wildfire - even a big one...\n",
    "# Let's make a project boundary!!! (source: https://fsapps.nwcg.gov/baer/baer-imagery-support-data-download))\n",
    "\n",
    "# to do this, we need to: \n",
    "# (1) make a new data folder \n",
    "# (2) download zips for burn severity data from the BAER website and NED data\n",
    "# (3) unzip files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) This will create a new folder \n",
    "\n",
    "zipfiles = \"zipfiles\"\n",
    "zippath = \"zipcontents\"\n",
    "reports = \"datareports\"\n",
    "newdata = \"newlayers\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(zippath)\n",
    "except OSError:\n",
    "    print (\"New folder creation for %s failed!!!\" % zipfiles)\n",
    "else:\n",
    "    print (\"Successfully created a folder for your .zip files , called %s \" % zipfiles)\n",
    "    \n",
    "try:\n",
    "    os.mkdir(zipfiles)\n",
    "except OSError:\n",
    "    print (\"New folder creation for %s failed!!!\" % zippath)\n",
    "else:\n",
    "    print (\"Successfully created a folder for your zip data, called %s \" % zippath)\n",
    "       \n",
    "\n",
    "try:\n",
    "    os.mkdir(newdata)\n",
    "except OSError:\n",
    "    print (\"New folder creation for %s failed!!!\" % newdata)\n",
    "else:\n",
    "    print (\"Successfully created a folder for additional layers you will create, called %s \" % newdata)\n",
    "    \n",
    "try:\n",
    "    os.mkdir(reports)\n",
    "except OSError:\n",
    "    print (\"New folder creation for %s failed!!!\" % reports)\n",
    "else:\n",
    "    print (\"Successfully created a folder for reports we will print later, called %s \" % reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) This downloads the zipped preliminary burn severity data from the USFS BAER site\n",
    "# so that we can use the layers in our project!\n",
    "\n",
    "zipurl = r'https://fsapps.nwcg.gov/baer/sites/baer/files/cameronpeak_co_preliminary_20200913.zip'\n",
    "DEMurl = r'https://www.coloradoview.org/wp-content/coloradoviewData/aerial/dem-4-4.zip'\n",
    "\n",
    "#files to be created\n",
    "BAERzip = os.path.join(zipfiles, 'BAERdata.zip')\n",
    "DEMzip = os.path.join(zipfiles, 'DEMdata.zip')\n",
    "\n",
    "r = requests.get(zipurl, allow_redirects=True)\n",
    "r2 = requests.get(DEMurl, allow_redirects=True)\n",
    "\n",
    "\n",
    "open(BAERzip, 'wb').write(r.content)\n",
    "open(DEMzip, 'wb').write(r2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) This unzips the downloaded data into the \"zipcontents\" folder we created\n",
    "\n",
    "with zipfile.ZipFile(BAERzip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(zippath)\n",
    "    \n",
    "with zipfile.ZipFile(DEMzip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(zippath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great!  Now we can bring in our freshly downloaded burn boundary layer\n",
    "# We will deal with elevation a little later\n",
    "\n",
    "burnpath = os.path.join(zippath, \"co4060910587920200813_20200809_20200913_burn_bndy_utm.shp\")\n",
    "burnbound = gpd.read_file(burnpath)\n",
    "\n",
    "\n",
    "burnbound.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to identify affected watersheds, including their respective streams and lakes\n",
    "# that may have been affected by the fire.\n",
    "\n",
    "# First, we need to make sure all dataframes are in the same CRS\n",
    "# We will use the CRS for the burn boundary as our reference CRS\n",
    "\n",
    "print(\"The CRS for burn boundary is \" + str(burnbound.crs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if COlakes.crs != burnbound.crs:\n",
    "    print(\"Uhoh! CRS mismatch between lakes and buffered boundary... let's fix that.\")\n",
    "    COlakes = COlakes.to_crs(burnbound.crs)\n",
    "    print(\"COlakes CRS is fixed and is now \" + str(COlakes.crs) )\n",
    "else:\n",
    "    print(\"CRS for both lakes and buffered burn boundary is a match.\")\n",
    "    \n",
    "if COstreams.crs != burnbound.crs:\n",
    "    print(\"\\nUhoh! CRS mismatch between rivers and buffered boundary... let's fix that.\")\n",
    "    COstreams = COstreams.to_crs(burnbound.crs)\n",
    "    print(\"COstreams CRS is fixed and is now \" + str(COstreams.crs) )\n",
    "else:\n",
    "    print(\"\\nCRS for both streams and buffered burn boundary is a match.\")\n",
    "    \n",
    "    \n",
    "if COws.crs != burnbound.crs:\n",
    "    print(\"\\nUhoh! CRS mismatch between watersheds and buffered boundary... let's fix that.\")\n",
    "    COws = COws.to_crs(burnbound.crs)\n",
    "    print(\"COws CRS is fixed and is now \" + str(COws.crs) )\n",
    "else:\n",
    "    print(\"\\nCRS for both watersheds and buffered burn boundary is a match.\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ....now clipping streams, lakes and watersheds to our area of interest...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't just clip watersheds by the burn boundary, because we want to see the whole watershed\n",
    "# that might be affected by the fire\n",
    "# solution?  First make a layer of all watersheds that intersect with the fire\n",
    "# unfortunately, we lose some of our pertinent data (i.e. watershed name) in this spatial join\n",
    "# don't worry.... i have a plan\n",
    "\n",
    "sheds_in_fire = gpd.sjoin(COws, burnbound, how=\"inner\", op='intersects')\n",
    "sheds_in_fire.plot()\n",
    "\n",
    "# sheds_in_fire shows only the watersheds where some part falls within the fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now clip the original CO watersheds layer by our 'sheds_in_fire'\n",
    "# thereby creating a layer with the unique name of each affected watershed\n",
    "# keep_geom_type = True prevents us from accidentally creating watersheds that are just boundary lines\n",
    "# polygonal watersheds only, please!\n",
    "\n",
    "sheds = gpd.clip(COws, sheds_in_fire, keep_geom_type=True)\n",
    "\n",
    "sheds.plot(cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which watersheds may have been affected by the Cameron Peak Fire?\n",
    "\n",
    "sheds_unique = sheds.Name.unique()\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for ws in sheds_unique:\n",
    "    counter = counter + 1\n",
    "    print(str(counter) + \". \" + ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Any reservoir within the watershed may be at risk of sedimentation from erosion within the fire\n",
    "# so we want to see any potentially affected streams and reservoirs\n",
    "# to remedy this, we will use our new watersheds layer to clip streams and lakes\n",
    "\n",
    "\n",
    "streams = gpd.clip(COstreams, sheds)\n",
    "lakes = gpd.clip(COlakes, sheds)\n",
    "\n",
    "streams.plot(figsize = (10,10))\n",
    "lakes.plot(figsize = (10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our clipped streams, lakes and watersheds layers to our new data folder, for future generations\n",
    "# CPF = Cameron Peak Fire \n",
    "\n",
    "streams.to_file(os.path.join(newdata, \"CPFstreams.shp\"))\n",
    "lakes.to_file(os.path.join(newdata, \"CPFlakes.shp\"))\n",
    "sheds.to_file(os.path.join(newdata, \"CPFwatersheds.shp\"))\n",
    "\n",
    "print(\"Shapefiles for clipped streams, lakes, and watersheds have been saved to your 'new layers' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's work with our dNBR and DEM rasters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's raster time!\n",
    "# here are the paths to: \n",
    "# 1. the elevation tif (source: Geospatial Data Gateway)\n",
    "# 2. DNBR burn severity tif (downloaded from BAER zip file earlier!!!)\n",
    "\n",
    "# set variables for our rasters\n",
    "elevrast = os.path.join(zippath, \"4_4.tif\")\n",
    "burnDNBR = os.path.join(zippath, \"co4060910587920200813_20200809_20200913_dnbr_utm.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's open them up and take a peak at the CRS\n",
    "\n",
    "elev = rasterio.open(elevrast)\n",
    "burnsevras = rasterio.open(burnDNBR)\n",
    "\n",
    "print(\"CRS for elevation is \" + str(elev.crs))\n",
    "print(\"CRS for burn severity is \" + str(burnsevras.crs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if these are in the same CRS\n",
    "\n",
    "if elev.crs == burnbound.crs and burnsevras.crs == burnbound.crs:\n",
    "    print(\"Good news, your two rasters and project area have the same CRS!\")\n",
    "    print(\"Elev CRS is \" + str(elev.crs) + \" and burn sev CRS is \" + str(burnsevras.crs))\n",
    "else:\n",
    "    print(\"Houston, we are mismatched.\")\n",
    "    elev = elev.to_crs(burnbound.crs)\n",
    "    burnsevras = burnsevras.to_crs(burnbound.crs)\n",
    "    print(\"Okay... now elev CRS is \" + str(elev.crs) + \" and burn sev CRS is \" + str(burnsevras.crs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well heck, what do these things look like?\n",
    "\n",
    "show(elev, cmap='terrain')\n",
    "show(burnsevras, cmap = 'jet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract by mask to cut both of these rasters down to the size of our project area\n",
    "# Note: burn sev raster needs to be extracted by mask to enable reclassification into burn sev categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert our mask feature (burnbound) into json format, to please the fickle rasterio gods\n",
    "\n",
    "def getFeatures(burnbound):\n",
    "    # now turning this into rasterio friendly layer\n",
    "    import json\n",
    "    return [json.loads(burnbound.to_json())['features'][0]['geometry']]\n",
    "\n",
    "coords = getFeatures(burnbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now... let's mask up\n",
    "\n",
    "# Clip the raster with burn boundary polygon\n",
    "camelev, out_transform = mask(dataset=elev, shapes=coords, crop = True)\n",
    "camburnsevras, out_transform = mask(dataset=burnsevras, shapes=coords, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how these turned out....\n",
    "\n",
    "show(camelev, cmap = 'terrain')\n",
    "show(camburnsevras, cmap = 'jet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update metadata for our new baby rasters\n",
    "\n",
    "\n",
    "#elev_meta=  elev.meta.copy()\n",
    "burn_meta=  burnsevras.meta.copy()\n",
    "elev_meta = elev.meta.copy()\n",
    "\n",
    "burnout = os.path.join(newdata, \"burnsev_clip.tif\")\n",
    "elevout = os.path.join(newdata, \"elev_clip.tif\")\n",
    "\n",
    "# now let's save this good stuff\n",
    "\n",
    "with rasterio.open(elevout, \"w\", **elev_meta) as dest:\n",
    "        dest.write(camelev)\n",
    "        print(\"A copy of your clipped elevation raster has been saved at \" + elevout)\n",
    "with rasterio.open(burnout, \"w\", **burn_meta) as dest:\n",
    "        dest.write(camburnsevras)\n",
    "        print(\"A copy of your clipped burn severity raster has been saved at \" + burnout)\n",
    "        \n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster reclassification time!\n",
    "#### NOTE: instead of using the AGOL hosted Cam Peak burn severity vector layer<br>I decided instead to:\n",
    "1. Use the raw dNBR .tif file from the BAER zip file we downloaded earlier\n",
    "2. Reclassify the dNBR file into <font color=green>*unburned* </font>, <font color=gold>*low* </font>, <font color=orange>*moderate* </font>, and <font color=red>*high*</font> severity burned areas\n",
    "3. Polygonize the reclassified raster\n",
    "4. Select out the high severity area\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to reclassify raster into our burn severity categories\n",
    "\n",
    "# RECLASS code adapted from an unknown user, but \n",
    "# the source: http://geoexamples.blogspot.com/2013/06/gdal-performance-raster-classification.html\n",
    "\n",
    "\n",
    "# To determine value cutoffs for different severity categories\n",
    "# I started with the suggestions on page LA-38 \n",
    "# from this document: https://www.fs.fed.us/rm/pubs/rmrs_gtr164/rmrs_gtr164_13_land_assess.pdf\n",
    "# and then adjusted them until it looked similar to the BAER data\n",
    "\n",
    "classification_values = [-692,100,500,800, 1130] #The interval values to classify  \n",
    "classification_output_values = [1,2,3,4,0] #The value assigned to each interval  \n",
    "  \n",
    "    \n",
    "in_file = burnout\n",
    "\n",
    "#out file\n",
    "\n",
    "burnreclass = os.path.join(newdata, \"burnsev_reclassed.tif\")\n",
    "\n",
    "ds = gdal.Open(in_file)\n",
    "band = ds.GetRasterBand(1)\n",
    "\n",
    "block_sizes = band.GetBlockSize()\n",
    "x_block_size = block_sizes[0]\n",
    "y_block_size = block_sizes[1]\n",
    "\n",
    "xsize = band.XSize\n",
    "ysize = band.YSize\n",
    "\n",
    "max_value = band.GetMaximum()\n",
    "min_value = band.GetMinimum()\n",
    "\n",
    "if max_value == None or min_value == None:\n",
    "    stats = band.GetStatistics(0, 1)\n",
    "    max_value = stats[1]\n",
    "    min_value = stats[0]\n",
    "\n",
    "format = \"GTiff\"\n",
    "driver = gdal.GetDriverByName( format )\n",
    "dst_ds = driver.Create(burnreclass, xsize, ysize, 1, gdal.GDT_Byte )\n",
    "dst_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "dst_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "for i in range(0, ysize, y_block_size):\n",
    "    if i + y_block_size < ysize:\n",
    "        rows = y_block_size\n",
    "    else:\n",
    "        rows = ysize - i\n",
    "    for j in range(0, xsize, x_block_size):\n",
    "        if j + x_block_size < xsize:\n",
    "            cols = x_block_size\n",
    "        else:\n",
    "            cols = xsize - j\n",
    "\n",
    "        data = band.ReadAsArray(j, i, cols, rows)\n",
    "        r = zeros((rows, cols), numpy.uint8)\n",
    "\n",
    "        for k in range(len(classification_values) - 1):\n",
    "            if classification_values[k] <= max_value and (classification_values[k + 1] > min_value ):\n",
    "                r = r + classification_output_values[k] * logical_and(data >= classification_values[k], data < classification_values[k + 1])\n",
    "        if classification_values[k + 1] < max_value:\n",
    "            r = r + classification_output_values[k+1] * (data >= classification_values[k + 1])\n",
    "\n",
    "        dst_ds.GetRasterBand(1).WriteArray(r,j,i)\n",
    "\n",
    "dst_ds = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This polygonizing code block adapted from Penn State\n",
    "# https://www.e-education.psu.edu/geog489/node/2215\n",
    "\n",
    "raster = gdal.Open(burnreclass)\n",
    "raster.RasterCount\n",
    "band = raster.GetRasterBand(1)\n",
    "\n",
    "burnpoly = os.path.join(newdata, \"burnsev_reclass_poly.shp\")\n",
    "\n",
    "drv = ogr.GetDriverByName('ESRI Shapefile')\n",
    "outfile = drv.CreateDataSource(burnpoly) \n",
    "outlayer = outfile.CreateLayer('polygonized raster', srs = None )\n",
    "newField = ogr.FieldDefn('DN', ogr.OFTReal)\n",
    "outlayer.CreateField(newField)\n",
    "\n",
    "\n",
    "# Once the shapefile is prepared, we call Polygonize(...) \n",
    "# and provide the band and the output layer as parameters plus a few additional parameters needed:\n",
    "\n",
    "\n",
    "gdal.Polygonize(band, None, outlayer, 0, [])\n",
    "outfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading polygonized raster into a geopandas dataframe\n",
    "\n",
    "burnpolygeo = gpd.read_file(burnpoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "burnpolygeo.plot(column=\"DN\", cmap= \"YlOrRd\", legend=True, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(burnpolygeo.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welp.  We need matching CRS with other layers before we can proceed\n",
    "\n",
    "burnpolygeo = burnpolygeo.set_crs(burnbound.crs)\n",
    "\n",
    "print(burnpolygeo.crs)\n",
    "burnpolygeo.to_file(os.path.join(newdata, \"BurnSeverityCategories.shp\"))\n",
    "print(\"A new shapefile showing categorized burn severity has been saved to your 'newlayers' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select out a new layer that is only High Severity (DN = 4.0)\n",
    "\n",
    "highsev = burnpolygeo[burnpolygeo.DN == 4.0]\n",
    "highsev.to_file(os.path.join(newdata, \"HighSevBurnedArea.shp\"))\n",
    "print(\"A new shapefile showing high severity burned area has been saved to your 'newlayers' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how it looks!\n",
    "\n",
    "highsev.plot(facecolor = \"DarkRed\", figsize =(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's hit the slopes!\n",
    "### I mean, convert elevation to slope\n",
    "#### but just for fun:\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3o7WIHvqsj3emiv3eE/giphy.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay... now about that elevation raster... what we really want is a slope raster\n",
    "# to identify higher risk areas of erosion \n",
    "# which we are calling slopes >30% in high severity areas \n",
    "\n",
    "# so... let's turn elevation into slope.\n",
    "# good thing we learned about richDEM, a package well suited for topographical raster magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# elevation to convert\n",
    "dem = rd.LoadGDAL(elevout)\n",
    "\n",
    "\n",
    "# resulting slope raster\n",
    "outslope = os.path.join(newdata, \"COslope_percent.tif\")\n",
    "\n",
    "\n",
    "slopey = rd.TerrainAttribute(dem, attrib='slope_percentage')\n",
    "rd.rdShow(slopey, axes=False, cmap='YlOrBr', figsize=(8,5.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save it!\n",
    "\n",
    "rd.SaveGDAL(outslope, slopey)\n",
    "print(\"A copy of the raw slope raster has been saved at \" + outslope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's reclassify into slopes >= 30% (high erosion risk) and < 30% (lower risk)\n",
    "\n",
    "# this RECLASS code adapted from an unknown user, but \n",
    "# the source: http://geoexamples.blogspot.com/2013/06/gdal-performance-raster-classification.html\n",
    "\n",
    "\n",
    "classification_values = [-10,30] #The interval values to classify  \n",
    "classification_output_values = [0,1] #The value assigned to each interval  \n",
    "  \n",
    "    \n",
    "in_file = outslope\n",
    "\n",
    "#out file\n",
    "\n",
    "slopereclass = os.path.join(newdata, \"sloperas_30.tif\")\n",
    "\n",
    "ds = gdal.Open(in_file)\n",
    "band = ds.GetRasterBand(1)\n",
    "\n",
    "block_sizes = band.GetBlockSize()\n",
    "x_block_size = block_sizes[0]\n",
    "y_block_size = block_sizes[1]\n",
    "\n",
    "xsize = band.XSize\n",
    "ysize = band.YSize\n",
    "\n",
    "max_value = band.GetMaximum()\n",
    "min_value = band.GetMinimum()\n",
    "\n",
    "if max_value == None or min_value == None:\n",
    "    stats = band.GetStatistics(0, 1)\n",
    "    max_value = stats[1]\n",
    "    min_value = stats[0]\n",
    "\n",
    "format = \"GTiff\"\n",
    "driver = gdal.GetDriverByName( format )\n",
    "dst_ds = driver.Create(slopereclass, xsize, ysize, 1, gdal.GDT_Byte )\n",
    "dst_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "dst_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "for i in range(0, ysize, y_block_size):\n",
    "    if i + y_block_size < ysize:\n",
    "        rows = y_block_size\n",
    "    else:\n",
    "        rows = ysize - i\n",
    "    for j in range(0, xsize, x_block_size):\n",
    "        if j + x_block_size < xsize:\n",
    "            cols = x_block_size\n",
    "        else:\n",
    "            cols = xsize - j\n",
    "\n",
    "        data = band.ReadAsArray(j, i, cols, rows)\n",
    "        r = zeros((rows, cols), numpy.uint8)\n",
    "\n",
    "        for k in range(len(classification_values) - 1):\n",
    "            if classification_values[k] <= max_value and (classification_values[k + 1] > min_value ):\n",
    "                r = r + classification_output_values[k] * logical_and(data >= classification_values[k], data < classification_values[k + 1])\n",
    "        if classification_values[k + 1] < max_value:\n",
    "            r = r + classification_output_values[k+1] * (data >= classification_values[k + 1])\n",
    "\n",
    "        dst_ds.GetRasterBand(1).WriteArray(r,j,i)\n",
    "\n",
    "dst_ds = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewreclass = rasterio.open(slopereclass)\n",
    "show(viewreclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this just shows that we have successfully reclassified into these 2 new values\n",
    "# everything > 25% slope has the DN (digital number) of 1, everything else <25% is 0\n",
    "\n",
    "show_hist(viewreclass, bins=50, lw=0.0, stacked=False, alpha=0.3,\n",
    "      histtype='stepfilled', title=\"Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to polygonize our binary slope, so we can create a geopandas dataframe\n",
    "# Of steep slopes to work with our other layers.\n",
    "\n",
    "\n",
    "# This code was successfully used to polygonize burn severity,\n",
    "# But failed horribly when trying to polygonize binary slope raster\n",
    "# Despite various tweaks, reclassifications, and exhaustively searching for and trying several other code snippets\n",
    "\n",
    "# This polygonizing code block adapted from Penn State\n",
    "# https://www.e-education.psu.edu/geog489/node/2215\n",
    "\n",
    "\n",
    "#band = raster.GetRasterBand(1)\n",
    "\n",
    "\n",
    "#slopepoly = os.path.join(newdata, \"slopere_poly.shp\") \n",
    "\n",
    "#drv = ogr.GetDriverByName('ESRI Shapefile')\n",
    "#outfile = drv.CreateDataSource(slopepoly) \n",
    "#outlayer = outfile.CreateLayer('polygonized raster', srs = None )\n",
    "#newField = ogr.FieldDefn('DN', ogr.OFTReal)\n",
    "#outlayer.CreateField(newField)\n",
    "\n",
    "\n",
    "#gdal.Polygonize(band, None, outlayer, 0, [])\n",
    "#outfile = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our workaround, you ask?!!!\n",
    "# Well, I converted the binary slope raster to vector format using ArcGIS Pro\n",
    "# Converted that layer to a geojson file\n",
    "# and am now hosting that geojson file (zipped) on Google Drive.\n",
    "#\n",
    "#\n",
    "#\n",
    "# Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the slope geojson I am hosting on google drive\n",
    "slopezipurl = r'https://drive.google.com/u/0/uc?id=1-61JkGOCo--NjWH4gcBXXzqAe5GXGL1i&export=download'\n",
    "\n",
    "slopezip = os.path.join(zipfiles, \"slope_binary.zip\")\n",
    "\n",
    "r = requests.get(slopezipurl, allow_redirects=True)\n",
    "\n",
    "open(slopezip, 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting zip file\n",
    "\n",
    "with zipfile.ZipFile(slopezip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(zippath)\n",
    "    print(\"Slope geojson has been saved in your zip folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading into geopandas\n",
    "\n",
    "slopegeo = os.path.join(zippath, \"slope_binary.geojson\")\n",
    "slopy = gpd.read_file(slopegeo)\n",
    "slopy.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopy.plot(column=\"gridcode\", cmap= \"YlOrBr\", legend=True, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select out a new layer that is only steep slopes (gridcode = 1)\n",
    "\n",
    "steep = slopy[slopy.gridcode == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steep.plot(facecolor = \"Orange\", figsize =(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we proceed with analyis, we have to do our favorite thing...\n",
    "# verifying CRS\n",
    "\n",
    "print(\"The current CRS for slope is \" + str(steep.crs))\n",
    "\n",
    "if steep.crs == burnbound.crs:\n",
    "    print(\"Good news, slope has the same CRS as your other layers!!\")\n",
    "    print(\"And that CRS is \" + str(steep.crs))\n",
    "else:\n",
    "    print(\"Houston, we are mismatched.  \")\n",
    "    steep = steep.to_crs(burnbound.crs)\n",
    "    print(\"Okay... now slope CRS has been converted to \" + str(steep.crs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point in the script, we have created the following layers:\n",
    " - ## <font color=lightblue>*streams* </font>\n",
    "     - accessed from online hosted layer\n",
    "     - clipped to burn area\n",
    " - ## <font color=blue>*lakes* </font>\n",
    "     - accessed from online hosted layer\n",
    "     - clipped to burn area\n",
    " - ## <font color=darkblue>*watersheds* </font>\n",
    "     - accessed from online hosted layer\n",
    "     - selected to include all sub watersheds that overlap with the burn area\n",
    " - ## <font color=orange>*Cameron Peak Fire burn boundary* </font>\n",
    "     - from downloaded/unzipped BAER data\n",
    " - ## <font color=red>*high severity burned area* </font>\n",
    "     - reclassified from raw dNBR from downloaded/unzipped BAER data\n",
    "     - categorized into severity classes\n",
    "     - polygonized\n",
    "     - new layer of 'high severity' created\n",
    " - ## <font color=brown>*steep slopes* </font>\n",
    "     - rendered from downloaded/unzipped DEM raster\n",
    "     - reclassified to show slopes >= 30%\n",
    "     - polygonized (via google file)\n",
    "     - new layer of 'steep slopes' created\n",
    "     \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Now let's get to work.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do high severity burn areas intersect with steep slopes?\n",
    "\n",
    "dangerzone = gpd.overlay(highsev, steep, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangerzone.plot(facecolor = \"Red\", figsize =(10,15))\n",
    "plt.title(\"Danger Zone: high severity burned area occuring on steep slopes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Oooooh boy.\n",
    "#  We should save this layer\n",
    "dangerzone.to_file(os.path.join(newdata, \"DangerZone.shp\"))\n",
    "print(\"A new shapefile showing the Danger Zone (high severity burn on steep slopes) is saved in the 'newlayers' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Which watersheds contain the dangerzone?\n",
    "\n",
    "dangersheds = gpd.sjoin(sheds, dangerzone, how=\"inner\", op='intersects')\n",
    "\n",
    "dangersheds.plot(cmap = 'plasma', figsize = (10,15))\n",
    "plt.title(\"Watersheds containing the Danger Zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now clip our fire watersheds layer by our dangersheds\n",
    "# thereby creating a layer with the unique name of each affected watershed\n",
    "# keep_geom_type = True prevents us from accidentally creating watersheds that are just boundary lines\n",
    "# polygonal watersheds only, please!\n",
    "\n",
    "affectedsheds = gpd.clip(sheds, dangersheds, keep_geom_type=True)\n",
    "\n",
    "affectedsheds.plot(cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which watersheds may have been affected by the Cameron Peak Fire?\n",
    "\n",
    "danger_unique = affectedsheds.Name.unique()\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for danger in danger_unique:\n",
    "    counter = counter + 1\n",
    "    print(str(counter) + \". \" + danger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow.  \n",
    "# Of the 18 watersheds within the fire, 16 contain some portion of overlapping high severity burn and steep slopes\n",
    "\n",
    "# Let's save that watershed layer\n",
    "affectedsheds.to_file(os.path.join(newdata, \"WatershedsInDangerZone.shp\"))\n",
    "print(\"A new shapefile showing watersheds containing high severity burn on steep slopes is now located in 'newlayers' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see where danger zone intersects with streams or lakes\n",
    "dangerstreams = gpd.sjoin(streams, dangerzone, how=\"inner\", op='intersects')\n",
    "dangerlakes = gpd.sjoin(lakes, dangerzone, how=\"inner\", op='intersects')\n",
    "\n",
    "dangerstreams.plot(cmap = 'Blues', figsize = (10,15))\n",
    "plt.title(\"Streams flowing through the danger zone\")\n",
    "\n",
    "# Note: this second plot will be blank. See comments in next cell. \n",
    "dangerlakes.plot(cmap = 'Blues', figsize = (10,10))\n",
    "plt.title(\"Lakes in the danger zone... wait a second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In retrospect, I suppose it makes sense that there are no lakes in areas with steep slopes... \n",
    "# Because the water would flow away.\n",
    "\n",
    "# Let's save that layer of streams, though\n",
    "dangerstreams.to_file(os.path.join(newdata, \"StreamsInDangerZone.shp\"))\n",
    "print(\"A new shapefile of streams that flow through high risk zones is saved in the 'newlayers' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which watersheds have streams that flowed directly through the danger zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to rename a troublesome field heading that prevents the join\n",
    "\n",
    "dangerstreams = dangerstreams.rename(columns={'index_right': 'horse'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamshed_risk = gpd.sjoin(affectedsheds, dangerstreams, how=\"left\", op='contains')\n",
    "\n",
    "affectedsheds.plot(cmap='plasma', figsize = (10,15))\n",
    "plt.title(\"Affected watersheds\")\n",
    "streamshed_risk.plot(cmap = 'jet', figsize = (10,15))\n",
    "plt.title(\"Affected watersheds with streams running directly through danger zone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well looks like every watershed that contains high severity burn and steep slopes \n",
    "#   also, unfortunately, has at least one stream running through it that passes through the danger zone\n",
    "#   What bodies of water lie within those watersheds?\n",
    "\n",
    "atrisk_lakes = gpd.clip(lakes, affectedsheds)\n",
    "atrisk_lakes.plot(cmap = 'Blues', figsize = (10,10))\n",
    "plt.title(\"Bodies of water in affected watersheds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrisk_lakes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the names \n",
    "\n",
    "lakerisk_unique = atrisk_lakes.name.unique()\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "\n",
    "for lago in lakerisk_unique:\n",
    "    if str(lago) != \"None\":\n",
    "        counter = counter + 1\n",
    "        print(str(counter) + \". \" + str(lago))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the shape area of these lakes to Hectares\n",
    "#And view the ten largest lakes\n",
    "\n",
    "\n",
    "\n",
    "atrisk_lakes['hectares'] = atrisk_lakes['geometry'].area/ 10000\n",
    "\n",
    "# Let's sort by the largest lakes\n",
    "print(\"Largest lakes in affected watersheds:\")\n",
    "atrisk_lakes.sort_values(by = \"hectares\", ascending=False).drop([\"co_fips\", \"shape_area\", \"shape_len\",\n",
    "                                                                \"geometry\"], axis = 1).head(15)\n",
    "          \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select out lakes with minimum surface of 10 hectares\n",
    "\n",
    "biglakes = atrisk_lakes[atrisk_lakes.hectares > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lakes at least 10 hectares in size\")\n",
    "biglakes.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's figure out the size of danger zone (high severity/steep) areas in each watershed\n",
    "\n",
    "# first, we need to clean out some unused columns, so that the join gives us just the data we need\n",
    "danger_simplify = dangerzone.drop([ 'FID', 'Id', 'gridcode'], axis=1)\n",
    "shed_simplify = affectedsheds.drop(['id', 'OBJECTID', 'TNMID', 'MetaSource', \"SourceData\", \"SourceOrig\", \"SourceFeat\", \"LoadDate\", \"GNIS_ID\",\"AreaAcres\", \"AreaSqKm\", \"HUC12\", \"HUType\", \"HUMod\", \"ToHUC\", \"NonContrib\", \"NonContr_1\", \"Shape_Leng\"], axis = 1)\n",
    "\n",
    "# Identifying danger zones in watersheds\n",
    "dangerinshed = gpd.sjoin(shed_simplify, danger_simplify, how= \"right\", op = \"intersects\")\n",
    "dangerinshed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to group together the high sev / steep areas by watershed\n",
    "shed_dissolve = dangerinshed.dissolve(by = \"Name\")\n",
    "\n",
    "# and calculate the affected area\n",
    "shed_dissolve['at_risk_area_hectares'] = shed_dissolve['geometry'].area/ 10000\n",
    "\n",
    "shed_dissolve.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have now identified:\n",
    " - <font color=red>**\"Danger Zone\"** </font> i.e. areas that burned at high severity occurring on steep slopes\n",
    " - <font color=darkblue>**watersheds** </font> that contain some portion of the Danger Zone\n",
    " - <font color=lightblue>**streams** </font> that run through the Danger Zone\n",
    " - <font color=blue>**lakes** </font> in watersheds containing the Danger Zone\n",
    " \n",
    " <br>\n",
    " \n",
    " \n",
    "### Our final tasks:\n",
    "  - print .csv file showing total Danger Zone area within each water shed\n",
    "  - print .csv file showing which lakes are in watersheds containing the Danger Zone\n",
    "  - make a nice plot that shows all of this data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the table so we can print a little report\n",
    "\n",
    "shed_pre_csv = shed_dissolve.drop(['geometry', 'index_left', \"States\", 'Shape_Area', \"DN\"], axis = 1)\n",
    "\n",
    "shed_pre_csv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shed_pre_csv.to_csv(os.path.join(reports,'AtRiskArea_byWatershed.csv'))\n",
    "print(\"This just in: We created a .csv file containing a list of affected watersheds, along with the total at-risk area in each watershed that burned at high severity and is located on steep slopes.\")\n",
    "print(\"\\nThis file is located in the reports folder we created earlier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save a list of the water bodies larger than 10 hectares as a .csv file\n",
    "\n",
    "risklakes_simplify = biglakes.drop(['co_fips', 'shape_area', 'shape_len'], axis=1)\n",
    "\n",
    "lakesindanger = gpd.sjoin(shed_simplify, risklakes_simplify, how= \"right\", op = \"intersects\")\n",
    "lakesindanger.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_precsv = lakesindanger.drop(['index_left', 'States', 'Shape_Area', 'geometry'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lakes_precsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_precsv.columns = ['Watershed', \"Lake_Name\", \"Lake_Area_hectares\"]\n",
    "lakes_precsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakesindanger.to_file(os.path.join(newdata, \"LakesinAffectedWatersheds.shp\"))\n",
    "lakes_precsv.to_csv(os.path.join(reports, r'AtRisk_Lakes.csv'))\n",
    "print(\"This just in: We created a .csv file containing a list of the lakes > 10 hectares) in affected watersheds.\")\n",
    "print(\"This file is also located in the 'reports' folder we created earlier.\")\n",
    "print(\"\\nWe also saved a shapefile of At Risk Lakes (in affected watersheds) in the 'newlayers' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to label the watersheds in our new figure\n",
    "\n",
    "    \n",
    "affectedsheds[\"center\"] = affectedsheds[\"geometry\"].centroid\n",
    "shed_points = affectedsheds.copy()\n",
    "shed_points.set_geometry(\"center\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code labels each watersheds\n",
    "# Adapted from...\n",
    "# Source:  https://github.com/shotleft/how-to-python/blob/master/How%20it%20works%20-%20labelling%20districts%20in%20GeoPandas.ipynb\n",
    "\n",
    "\n",
    "\n",
    "ax = affectedsheds.plot(figsize = (15, 12), color = \"whitesmoke\", edgecolor = \"lightgrey\", linewidth = 0.5)\n",
    "\n",
    "\n",
    "# sets aspect to equal. This is done automatically\n",
    "# when using *geopandas* plot on it's own, but not when\n",
    "# working with pyplot directly.  \n",
    "# Source:  https://geopandas.org/mapping.html\n",
    "\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Lakes, At-risk Streams, and High Severity Burn area on Steep Slopes in Watersheds of Cameron Peak Fire')\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "for x, y, label in zip(shed_points.geometry.x, shed_points.geometry.y, shed_points[\"Name\"]):\n",
    "    texts.append(plt.text(x, y, label, fontsize = 8))\n",
    "\n",
    "aT.adjust_text(texts, force_points=0.3, force_text=0.8, expand_points=(1,1), expand_text=(1,1), \n",
    "               arrowprops=dict(arrowstyle=\"-\", color='grey', lw=0.5))\n",
    "\n",
    "# hide axis ticks/text\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "# Now adding all lakes in affected sheds, but only the streams that run through the high-risk areas\n",
    "\n",
    "dangerstreams.plot(ax=ax, marker = 'o',  color = 'cornflowerblue', markersize =10)\n",
    "\n",
    "\n",
    "atrisk_lakes.plot(ax=ax, marker='o',  color='cornflowerblue', markersize=5)\n",
    "\n",
    "\n",
    "dangerzone.plot(ax=ax, marker='o', color='red',  markersize=5)\n",
    "\n",
    "# let's save this plot as well \n",
    "plt.savefig(os.path.join(reports, 'DangerZone.pdf'))  \n",
    "print(\"We saved a copy of this map in your reports folder, for your viewing pleasure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look in the original folder where the .ipynb notebook file is located<br>You should see:\n",
    " - zip files for the DEM, BAER burn severity, and polygonized slope data downloaded from the internet\n",
    " - \"zipcontents\" folder containing all data that was unzipped from those files\n",
    " - \"newlayers\" folder containing .tif and .shp files created during the script that could be useful in the future\n",
    " - \"reports\" folder containing \n",
    "1. a lovely .pdf showing <font color=darkblue>**watersheds** </font>  in the <font color=red>**\"Danger Zone\"**</font>, <font color=lightblue>**streams** </font> that run through high risk areas, and <font color=blue>**lakes** </font> within the affected areas\n",
    "2. a .csv report of total high severity / steep slope area within each watershed\n",
    "3. a .csv report of lakes larger than 10 hectares within affected watersheds\n",
    "\n",
    "     \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l3vRaWnqG3gOZ8lsk/giphy.gif\" />\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
